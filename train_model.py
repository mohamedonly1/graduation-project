#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
===========================================
ØªØ¯Ø±ÙŠØ¨ Ù…ÙˆØ¯ÙŠÙ„ Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ÙˆØ­Ø¯Ø©
===========================================
Ø´ØºÙ‘Ù„ Ø§Ù„Ù…Ù„Ù Ø¯Ù‡ Ø¨Ø¹Ø¯ Ù…Ø§ ØªØ¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù€ collect_data.py
"""

import csv
import numpy as np
import os

# =============================================
# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# =============================================
print("=" * 50)
print("  ØªØ¯Ø±ÙŠØ¨ Ù…ÙˆØ¯ÙŠÙ„ Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©")
print("=" * 50)

DATA_PATH = 'arabic_data/arabic_keypoints.csv'
LABELS_PATH = 'arabic_data/arabic_labels.csv'
MODEL_SAVE_PATH = 'arabic_model'

os.makedirs(MODEL_SAVE_PATH, exist_ok=True)

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª
labels_dict = {}
with open(LABELS_PATH, 'r', encoding='utf-8') as f:
    reader = csv.reader(f)
    for row in reader:
        labels_dict[int(row[0])] = row[1]

print(f"\nğŸ“‚ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ù…ØªØ§Ø­Ø©: {len(labels_dict)}")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
X, y = [], []
with open(DATA_PATH, 'r') as f:
    reader = csv.reader(f)
    for row in reader:
        if row:
            y.append(int(row[0]))
            X.append([float(v) for v in row[1:]])

X = np.array(X)
y = np.array(y)

print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(X)} Ø¹ÙŠÙ†Ø©")
print(f"ğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª:")

unique, counts = np.unique(y, return_counts=True)
for label, count in zip(unique, counts):
    letter = labels_dict.get(label, '?')
    bar = "â–ˆ" * (count // 10)
    status = "âœ…" if count >= 200 else "âš ï¸ "
    print(f"  {status} {letter}: {count} {bar}")

# =============================================
# 2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
# =============================================
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

num_classes = len(np.unique(y))
print(f"\nğŸ”¢ Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {num_classes}")

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"âœ… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨: {len(X_train)}")
print(f"âœ… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {len(X_test)}")

# =============================================
# 3. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
# =============================================
import tensorflow as tf
from tensorflow import keras

print("\nğŸ—ï¸  Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„...")

model = keras.Sequential([
    keras.layers.Input(shape=(42,)),                          # 21 Ù†Ù‚Ø·Ø© Ã— 2 (x,y)

    keras.layers.Dense(128, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.3),

    keras.layers.Dense(256, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.3),

    keras.layers.Dense(128, activation='relu'),
    keras.layers.BatchNormalization(),
    keras.layers.Dropout(0.2),

    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.2),

    keras.layers.Dense(num_classes, activation='softmax')
])

model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# =============================================
# 4. Callbacks
# =============================================
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=20,
        restore_best_weights=True,
        verbose=1
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=10,
        min_lr=1e-6,
        verbose=1
    ),
    keras.callbacks.ModelCheckpoint(
        filepath=os.path.join(MODEL_SAVE_PATH, 'best_model.h5'),
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
]

# =============================================
# 5. Ø§Ù„ØªØ¯Ø±ÙŠØ¨
# =============================================
print("\nğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")

history = model.fit(
    X_train, y_train,
    epochs=200,
    batch_size=32,
    validation_split=0.2,
    callbacks=callbacks,
    verbose=1
)

# =============================================
# 6. Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
# =============================================
print("\nğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:")
test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
print(f"  âœ… Accuracy: {test_acc * 100:.2f}%")
print(f"  ğŸ“‰ Loss: {test_loss:.4f}")

# =============================================
# 7. Confusion Matrix
# =============================================
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

y_pred = np.argmax(model.predict(X_test), axis=1)

print("\nğŸ“‹ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ:")
letter_names = [labels_dict.get(i, str(i)) for i in range(num_classes)]
print(classification_report(y_test, y_pred, target_names=letter_names))

# Ø±Ø³Ù… Confusion Matrix
plt.figure(figsize=(16, 14))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=letter_names,
            yticklabels=letter_names)
plt.title('Confusion Matrix - Ù„ØºØ© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©', fontsize=16)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.tight_layout()
plt.savefig(os.path.join(MODEL_SAVE_PATH, 'confusion_matrix.png'), dpi=150)
plt.show()
print(f"âœ… ØªÙ… Ø­ÙØ¸ Confusion Matrix")

# Ø±Ø³Ù… Ù…Ù†Ø­Ù†ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

ax1.plot(history.history['accuracy'], label='Train Accuracy')
ax1.plot(history.history['val_accuracy'], label='Val Accuracy')
ax1.set_title('Model Accuracy')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Accuracy')
ax1.legend()
ax1.grid(True)

ax2.plot(history.history['loss'], label='Train Loss')
ax2.plot(history.history['val_loss'], label='Val Loss')
ax2.set_title('Model Loss')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Loss')
ax2.legend()
ax2.grid(True)

plt.tight_layout()
plt.savefig(os.path.join(MODEL_SAVE_PATH, 'training_curves.png'), dpi=150)
plt.show()
print(f"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù†Ø­Ù†ÙŠØ§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨")

# =============================================
# 8. Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¨ØµÙŠØºØ© TFLite
# =============================================
print("\nğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„...")

# Ø­ÙØ¸ H5
model.save(os.path.join(MODEL_SAVE_PATH, 'arabic_sign_model.h5'))

# ØªØ­ÙˆÙŠÙ„ Ù„Ù€ TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

tflite_path = os.path.join(MODEL_SAVE_PATH, 'arabic_sign_model.tflite')
with open(tflite_path, 'wb') as f:
    f.write(tflite_model)

print(f"âœ… ØªÙ… Ø§Ù„Ø­ÙØ¸:")
print(f"   - {MODEL_SAVE_PATH}/arabic_sign_model.h5")
print(f"   - {MODEL_SAVE_PATH}/arabic_sign_model.tflite")
print(f"   - {MODEL_SAVE_PATH}/confusion_matrix.png")
print(f"   - {MODEL_SAVE_PATH}/training_curves.png")
print(f"\nğŸ‰ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨! Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {test_acc * 100:.2f}%")
